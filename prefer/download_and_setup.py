#!/usr/bin/env python3
"""
SFTæ¨¡å‹å’Œæ•°æ®é›†ä¸‹è½½é…ç½®è„šæœ¬
è‡ªåŠ¨ä¸‹è½½RTOçš„SFTæ¨¡å‹å’ŒUltraFeedbackæ•°æ®é›†ï¼Œå¹¶è½¬æ¢ä¸ºOpenRLHFæ ¼å¼
"""

import os
import json
import argparse
from pathlib import Path
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


def download_sft_model(model_name="OpenRLHF/Llama-3-8b-sft-mixture", save_dir="./models"):
    """
    ä¸‹è½½SFTæ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°
        save_dir: ä¿å­˜ç›®å½•
    """
    print(f"æ­£åœ¨ä¸‹è½½SFTæ¨¡å‹: {model_name}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    # ä½¿ç”¨Princetonå‰ç¼€é¿å…è¦†ç›–åŸæœ‰æ¨¡å‹
    model_path = os.path.join(save_dir, f"Princeton_{model_name.split('/')[-1]}")
    
    if os.path.exists(model_path):
        print(f"âœ… æ¨¡å‹å·²å­˜åœ¨: {model_path}")
        return model_path
    
    try:
        # ä¸‹è½½tokenizer
        print("ä¸‹è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.save_pretrained(model_path)
        
        # ä¸‹è½½æ¨¡å‹ï¼ˆä»…ä¸‹è½½é…ç½®ï¼Œä¸ä¸‹è½½æƒé‡ä»¥èŠ‚çœæ—¶é—´ï¼‰
        print("ä¸‹è½½æ¨¡å‹é…ç½®...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        model.save_pretrained(model_path)
        
        print(f"âœ… SFTæ¨¡å‹ä¸‹è½½å®Œæˆ: {model_path}")
        return model_path
        
    except Exception as e:
        print(f"âŒ SFTæ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        return None


def download_ultrafeedback_dataset(dataset_name="princeton-nlp/llama3-ultrafeedback-armorm", save_dir="./data"):
    """
    ä¸‹è½½Llama3-UltraFeedback-ArmoRMæ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        save_dir: ä¿å­˜ç›®å½•
    """
    print(f"æ­£åœ¨ä¸‹è½½Llama3-UltraFeedback-ArmoRMæ•°æ®é›†: {dataset_name}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    try:
        # ä¸‹è½½æ•°æ®é›† - ä½¿ç”¨train split
        dataset = load_dataset(dataset_name, split="train")
        print(f"âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼ŒåŒ…å« {len(dataset)} ä¸ªæ ·æœ¬")
        
        # ä¿å­˜åˆ°æœ¬åœ° - ä½¿ç”¨Princetonå‰ç¼€é¿å…è¦†ç›–åŸæœ‰æ•°æ®
        dataset_path = os.path.join(save_dir, "Princeton_llama3_ultrafeedback_armorm")
        dataset.save_to_disk(dataset_path)
        print(f"âœ… æ•°æ®é›†ä¿å­˜åˆ°: {dataset_path}")
        
        return dataset_path
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}")
        return None


def convert_to_openrlhf_format(dataset_path, output_file, max_samples=None):
    """
    è½¬æ¢ä¸ºOpenRLHF DPOæ ¼å¼
    
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶
        max_samples: æœ€å¤§æ ·æœ¬æ•°
    """
    print("è½¬æ¢ä¸ºOpenRLHF DPOæ ¼å¼...")
    
    try:
        # ç›´æ¥ä»ç£ç›˜åŠ è½½æ•°æ®é›†
        from datasets import load_from_disk
        dataset = load_from_disk(dataset_path)
        
        dpo_data = []
        sample_count = 0
        
        for sample in dataset:
            if max_samples and sample_count >= max_samples:
                break
                
            # Llama3-UltraFeedback-ArmoRMæ•°æ®é›†çš„æ ¼å¼ï¼š
            # - prompt: ç”¨æˆ·è¾“å…¥
            # - chosen: æ›´å¥½çš„å›ç­”ï¼ˆå¯¹è¯æ ¼å¼ï¼‰
            # - rejected: è¾ƒå·®çš„å›ç­”ï¼ˆå¯¹è¯æ ¼å¼ï¼‰
            
            if 'prompt' in sample and 'chosen' in sample and 'rejected' in sample:
                # æå–promptï¼ˆç”¨æˆ·æ¶ˆæ¯ï¼‰
                prompt = sample['prompt']
                
                # æå–chosenå’Œrejectedçš„assistantå›å¤
                chosen_response = ""
                rejected_response = ""
                
                # å¤„ç†chosenå›å¤ - æ–°æ•°æ®é›†ä½¿ç”¨å¯¹è¯æ ¼å¼
                if isinstance(sample['chosen'], list):
                    for msg in sample['chosen']:
                        if msg.get('role') == 'assistant':
                            chosen_response = msg.get('content', '')
                            break
                else:
                    chosen_response = str(sample['chosen'])
                
                # å¤„ç†rejectedå›å¤ - æ–°æ•°æ®é›†ä½¿ç”¨å¯¹è¯æ ¼å¼
                if isinstance(sample['rejected'], list):
                    for msg in sample['rejected']:
                        if msg.get('role') == 'assistant':
                            rejected_response = msg.get('content', '')
                            break
                else:
                    rejected_response = str(sample['rejected'])
                
                # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                if chosen_response and rejected_response and chosen_response != rejected_response:
                    # åˆ›å»ºDPOæ ¼å¼æ ·æœ¬
                    dpo_sample = {
                        'prompt': prompt,
                        'chosen': chosen_response,
                        'rejected': rejected_response
                    }
                    dpo_data.append(dpo_sample)
                    sample_count += 1
                else:
                    print(f"è·³è¿‡æ— æ•ˆæ ·æœ¬ {sample_count}: chosenå’Œrejectedç›¸åŒæˆ–ä¸ºç©º")
                    continue
                
            else:
                print(f"è·³è¿‡æ ¼å¼ä¸åŒ¹é…çš„æ ·æœ¬: {sample.keys()}")
                continue
        
        # ä¿å­˜ä¸ºJSONLæ ¼å¼
        with open(output_file, 'w', encoding='utf-8') as f:
            for sample in dpo_data:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        print(f"âœ… è½¬æ¢å®Œæˆï¼Œå…± {len(dpo_data)} ä¸ªæ ·æœ¬ä¿å­˜åˆ°: {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ æ ¼å¼è½¬æ¢å¤±è´¥: {e}")
        return False


def create_config_file(model_path, dataset_path, output_dir="./config"):
    """
    åˆ›å»ºOpenRLHFé…ç½®æ–‡ä»¶
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        dataset_path: æ•°æ®é›†è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
    """
    print("åˆ›å»ºOpenRLHFé…ç½®æ–‡ä»¶...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    config = {
        "model": {
            "pretrain": model_path,
            "ref_pretrain": model_path
        },
        "dataset": {
            "path": dataset_path,
            "split": "train",
            "max_samples": 100000
        },
        "training": {
            "train_batch_size": 128,
            "micro_train_batch_size": 4,
            "max_epochs": 3,
            "max_len": 1024,
            "learning_rate": 5e-7,
            "beta": 0.1
        },
        "optimization": {
            "zero_stage": 2,
            "adam_offload": True,
            "flash_attn": True,
            "gradient_checkpointing": True
        },
        "logging": {
            "save_path": "./checkpoint/Princeton_llama3-8b-dpo-ultrafeedback-armorm",
            "save_steps": 500,
            "logging_steps": 10,
            "eval_steps": 200,
            "use_wandb": True,
            "wandb_project": "princeton_dpo",
            "wandb_run_name": "Princeton_llama3-8b-dpo-ultrafeedback-armorm"
        },
        "gpu": {
            "cuda_visible_devices": "0,1,2,3"
        }
    }
    
    config_file = os.path.join(output_dir, "Princeton_openrlhf_config.json")
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… é…ç½®æ–‡ä»¶ä¿å­˜åˆ°: {config_file}")
    return config_file


def main():
    parser = argparse.ArgumentParser(description="ä¸‹è½½å’Œé…ç½®SFTæ¨¡å‹å’Œæ•°æ®é›†")
    parser.add_argument("--model_name", type=str, default="OpenRLHF/Llama-3-8b-sft-mixture",
                       help="SFTæ¨¡å‹åç§°")
    parser.add_argument("--dataset_name", type=str, default="princeton-nlp/llama3-ultrafeedback-armorm",
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--model_dir", type=str, default="./models",
                       help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument("--data_dir", type=str, default="./data",
                       help="æ•°æ®ä¿å­˜ç›®å½•")
    parser.add_argument("--config_dir", type=str, default="./config",
                       help="é…ç½®ä¿å­˜ç›®å½•")
    parser.add_argument("--max_samples", type=int, default=100000,
                       help="æœ€å¤§æ ·æœ¬æ•°é‡")
    parser.add_argument("--skip_model", action="store_true",
                       help="è·³è¿‡æ¨¡å‹ä¸‹è½½")
    parser.add_argument("--skip_dataset", action="store_true",
                       help="è·³è¿‡æ•°æ®é›†ä¸‹è½½")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("SFTæ¨¡å‹å’Œæ•°æ®é›†ä¸‹è½½é…ç½®è„šæœ¬")
    print("=" * 60)
    
    # ä¸‹è½½SFTæ¨¡å‹
    model_path = None
    if not args.skip_model:
        model_path = download_sft_model(args.model_name, args.model_dir)
        if not model_path:
            print("âŒ SFTæ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œé€€å‡º")
            return
    
    # ä¸‹è½½æ•°æ®é›†
    dataset_path = None
    if not args.skip_dataset:
        dataset_path = download_ultrafeedback_dataset(args.dataset_name, args.data_dir)
        if not dataset_path:
            print("âŒ æ•°æ®é›†ä¸‹è½½å¤±è´¥ï¼Œé€€å‡º")
            return
    
    # è½¬æ¢æ•°æ®é›†æ ¼å¼
    if dataset_path:
        output_file = os.path.join(args.data_dir, "Princeton_llama3_ultrafeedback_armorm_dpo.jsonl")
        if not convert_to_openrlhf_format(dataset_path, output_file, args.max_samples):
            print("âŒ æ•°æ®é›†æ ¼å¼è½¬æ¢å¤±è´¥ï¼Œé€€å‡º")
            return
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    if model_path and dataset_path:
        config_file = create_config_file(model_path, output_file, args.config_dir)
        print(f"âœ… é…ç½®å®Œæˆï¼é…ç½®æ–‡ä»¶: {config_file}")
    
    print("=" * 60)
    print("ğŸ‰ æ‰€æœ‰é…ç½®å®Œæˆï¼")
    print("ç°åœ¨å¯ä»¥è¿è¡ŒDPOè®­ç»ƒ:")
    print("bash train_dpo_rto_sft_improved.sh")
    print("=" * 60)


if __name__ == "__main__":
    main()
